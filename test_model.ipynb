{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMQj555bTXgPk4ZXDWKyxqI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElomKossi/Traffic_Signs_Recogniction/blob/main/test_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G-Xpxh3O7ve"
      },
      "source": [
        "# Test the modem "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHLzQ7JHPM77"
      },
      "source": [
        "**Connect google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sM025ZuO4Vo",
        "outputId": "ecdaa58f-d0ee-4e2a-eb09-242048d707e9"
      },
      "source": [
        "# Check if NVIDIA GPU is enabled\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec  9 12:08:28 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww1UTVcmPQIP",
        "outputId": "c2c62f10-2312-4cde-fd55-a583e01aad5f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "'Colab Notebooks'\t    'Secret Story'\t UTBM_Printemps_2019\n",
            " machine-learning-training  'Stage ST40'\t UTBM_Printemps_2020\n",
            "'My Drive'\t\t     UTBM_Automne_2018\t yoloV3\n",
            " Reglement.pdf\t\t     UTBM_Automne_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkz1MxTXPewn"
      },
      "source": [
        "**1) Importing needed libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIrm-0JTPhYm"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import cv2\n",
        "import time\n",
        "from timeit import default_timer as timer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLF7UtdJQs86",
        "outputId": "d907a7e4-0413-41cc-fe19-55ee2fc442c3"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/yoloV3/darknet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/yoloV3/darknet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koaOYTsOb1uH"
      },
      "source": [
        "**2) Reading input image / video or stream video from camera**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Xm9qk2cF4_"
      },
      "source": [
        "**2.1) Reading input image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHaD31V1cUxQ"
      },
      "source": [
        "image_BGR = cv2.imread('/content/gdrive/MyDrive/yoloV3/data/images/ XXX ')\n",
        "\n",
        "# Showing Original Image\n",
        "# Giving name to the window with Original Image\n",
        "# And specifying that window is resizable\n",
        "cv2.namedWindow('Original Image', cv2.WINDOW_NORMAL)\n",
        "# Pay attention! 'cv2.imshow' takes images in BGR format\n",
        "cv2.imshow('Original Image', image_BGR)\n",
        "# Waiting for any key being pressed\n",
        "cv2.waitKey(0)\n",
        "# Destroying opened window with name 'Original Image'\n",
        "cv2.destroyWindow('Original Image')\n",
        "\n",
        "# # Check point\n",
        "# # Showing image shape\n",
        "# print('Image shape:', image_BGR.shape) \n",
        "\n",
        "# Getting spatial dimension of input image\n",
        "h, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n",
        "\n",
        "# # Check point\n",
        "# # Showing height an width of image\n",
        "# print('Image height={0} and width={1}'.format(h, w)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-rX0T9pcLf1"
      },
      "source": [
        "**2.2) Reading input video**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PqRIl4NdIdd"
      },
      "source": [
        "video = cv2.VideoCapture('/content/gdrive/MyDrive/yoloV3/data/videos/ XXX ')\n",
        "\n",
        "# Preparing variable for writer\n",
        "# that we will use to write processed frames\n",
        "writer = None\n",
        "\n",
        "# Preparing variables for spatial dimensions of the frames\n",
        "h, w = None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B891eSRcL-8"
      },
      "source": [
        "**2.3) Reading stream video from camera**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBU2j8VhdVnP"
      },
      "source": [
        "# Defining 'VideoCapture' object\n",
        "# and reading stream video from camera\n",
        "camera = cv2.VideoCapture(0)\n",
        "\n",
        "# Preparing variables for spatial dimensions of the frames\n",
        "h, w = None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH7FcryeQK23"
      },
      "source": [
        "**3) Loading labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gT28zNbPuI5",
        "outputId": "c29714b4-207f-4796-9c37-e45c9435dd07"
      },
      "source": [
        "labels = pd.read_csv(\"/content/gdrive/MyDrive/yoloV3/darknet/data/ts_data/signnames.csv\")\n",
        "\n",
        "# Check point\n",
        "# Showing first 5 rows from the dataFrame\n",
        "print(labels.head())\n",
        "print()\n",
        "\n",
        "# To locate by class number use one of the following\n",
        "# ***.iloc[0][1] - returns element on the 0 column and 1 row\n",
        "print(labels.iloc[0][1])  # Speed limit (20km/h)\n",
        "# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\n",
        "print(labels['SignName'][1]) # Speed limit (30km/h)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   ClassId              SignName\n",
            "0        0  Speed limit (20km/h)\n",
            "1        1  Speed limit (30km/h)\n",
            "2        2  Speed limit (50km/h)\n",
            "3        3  Speed limit (60km/h)\n",
            "4        4  Speed limit (70km/h)\n",
            "\n",
            "Speed limit (20km/h)\n",
            "Speed limit (30km/h)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOT69-tETHxS"
      },
      "source": [
        "**4) Loading YOLO v3 network by OpenCV dnn library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKFHxjuhTgc6"
      },
      "source": [
        "**4.1) Loading trained weights and cfg file into the Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxnD45gHRDg8"
      },
      "source": [
        "path_to_weights = '/content/gdrive/MyDrive/yoloV3/darknet/backup/yolov3_ts_train_11000.weights'\n",
        "path_to_cfg = '/content/gdrive/MyDrive/yoloV3/darknet/cfg/yolov3_ts_test.cfg'\n",
        "\n",
        "# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\n",
        "network = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n",
        "\n",
        "# To use with GPU\n",
        "network.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "network.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35EM-7SzYFdo"
      },
      "source": [
        "**4.2) Getting output layers where detections are made**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rNRwfSNYUMK",
        "outputId": "a3a7ff91-af59-4616-f692-e16928660975"
      },
      "source": [
        "# Getting list with names of all layers from YOLO v3 network\n",
        "layers_names_all = network.getLayerNames()\n",
        "\n",
        "# # Check point\n",
        "print()\n",
        "print(layers_names_all)\n",
        "\n",
        "# Getting only output layers' names that we need from YOLO v3 algorithm\n",
        "# with function that returns indexes of layers with unconnected outputs\n",
        "layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
        "\n",
        "# Check point\n",
        "print()\n",
        "print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "['conv_0', 'bn_0', 'relu_0', 'conv_1', 'bn_1', 'relu_1', 'conv_2', 'bn_2', 'relu_2', 'conv_3', 'bn_3', 'relu_3', 'shortcut_4', 'conv_5', 'bn_5', 'relu_5', 'conv_6', 'bn_6', 'relu_6', 'conv_7', 'bn_7', 'relu_7', 'shortcut_8', 'conv_9', 'bn_9', 'relu_9', 'conv_10', 'bn_10', 'relu_10', 'shortcut_11', 'conv_12', 'bn_12', 'relu_12', 'conv_13', 'bn_13', 'relu_13', 'conv_14', 'bn_14', 'relu_14', 'shortcut_15', 'conv_16', 'bn_16', 'relu_16', 'conv_17', 'bn_17', 'relu_17', 'shortcut_18', 'conv_19', 'bn_19', 'relu_19', 'conv_20', 'bn_20', 'relu_20', 'shortcut_21', 'conv_22', 'bn_22', 'relu_22', 'conv_23', 'bn_23', 'relu_23', 'shortcut_24', 'conv_25', 'bn_25', 'relu_25', 'conv_26', 'bn_26', 'relu_26', 'shortcut_27', 'conv_28', 'bn_28', 'relu_28', 'conv_29', 'bn_29', 'relu_29', 'shortcut_30', 'conv_31', 'bn_31', 'relu_31', 'conv_32', 'bn_32', 'relu_32', 'shortcut_33', 'conv_34', 'bn_34', 'relu_34', 'conv_35', 'bn_35', 'relu_35', 'shortcut_36', 'conv_37', 'bn_37', 'relu_37', 'conv_38', 'bn_38', 'relu_38', 'conv_39', 'bn_39', 'relu_39', 'shortcut_40', 'conv_41', 'bn_41', 'relu_41', 'conv_42', 'bn_42', 'relu_42', 'shortcut_43', 'conv_44', 'bn_44', 'relu_44', 'conv_45', 'bn_45', 'relu_45', 'shortcut_46', 'conv_47', 'bn_47', 'relu_47', 'conv_48', 'bn_48', 'relu_48', 'shortcut_49', 'conv_50', 'bn_50', 'relu_50', 'conv_51', 'bn_51', 'relu_51', 'shortcut_52', 'conv_53', 'bn_53', 'relu_53', 'conv_54', 'bn_54', 'relu_54', 'shortcut_55', 'conv_56', 'bn_56', 'relu_56', 'conv_57', 'bn_57', 'relu_57', 'shortcut_58', 'conv_59', 'bn_59', 'relu_59', 'conv_60', 'bn_60', 'relu_60', 'shortcut_61', 'conv_62', 'bn_62', 'relu_62', 'conv_63', 'bn_63', 'relu_63', 'conv_64', 'bn_64', 'relu_64', 'shortcut_65', 'conv_66', 'bn_66', 'relu_66', 'conv_67', 'bn_67', 'relu_67', 'shortcut_68', 'conv_69', 'bn_69', 'relu_69', 'conv_70', 'bn_70', 'relu_70', 'shortcut_71', 'conv_72', 'bn_72', 'relu_72', 'conv_73', 'bn_73', 'relu_73', 'shortcut_74', 'conv_75', 'bn_75', 'relu_75', 'conv_76', 'bn_76', 'relu_76', 'conv_77', 'bn_77', 'relu_77', 'conv_78', 'bn_78', 'relu_78', 'conv_79', 'bn_79', 'relu_79', 'conv_80', 'bn_80', 'relu_80', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'relu_84', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'relu_87', 'conv_88', 'bn_88', 'relu_88', 'conv_89', 'bn_89', 'relu_89', 'conv_90', 'bn_90', 'relu_90', 'conv_91', 'bn_91', 'relu_91', 'conv_92', 'bn_92', 'relu_92', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'relu_96', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'relu_99', 'conv_100', 'bn_100', 'relu_100', 'conv_101', 'bn_101', 'relu_101', 'conv_102', 'bn_102', 'relu_102', 'conv_103', 'bn_103', 'relu_103', 'conv_104', 'bn_104', 'relu_104', 'conv_105', 'permute_106', 'yolo_106']\n",
            "\n",
            "['yolo_82', 'yolo_94', 'yolo_106']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOBtAYwsaeOt"
      },
      "source": [
        "**4.3) Setting probability, threshold and colour for bounding boxes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfcaJJLSag6q",
        "outputId": "9e0e7f6b-400c-4a0f-9a8b-365bd3334fb2"
      },
      "source": [
        "# Minimum probability to eliminate weak detections\n",
        "probability_minimum = 0.5\n",
        "\n",
        "# Setting threshold to filtering weak bounding boxes by non-maximum suppression\n",
        "threshold = 0.3\n",
        "\n",
        "# Generating colours for bounding boxes\n",
        "# randint(low, high=None, size=None, dtype='l')\n",
        "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
        "\n",
        "# Check point\n",
        "print(type(colours))  # <class 'numpy.ndarray'>\n",
        "print(colours.shape)  # (43, 3)\n",
        "print(colours[0])  # [25  65 200]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(43, 3)\n",
            "[ 3 74 71]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBNSGBxZeR7p"
      },
      "source": [
        "**5) Reading frames in the loop (for video)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq64EeMJddMP"
      },
      "source": [
        "# Defining variable for counting frames\n",
        "# At the end we will show total amount of processed frames\n",
        "f = 0\n",
        "\n",
        "# Defining variable for counting total time\n",
        "# At the end we will show time spent for processing all frames\n",
        "t = 0\n",
        "\n",
        "# Defining loop for catching frames\n",
        "while True:\n",
        "    # Capturing frame-by-frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # If the frame was not retrieved\n",
        "    # e.g.: at the end of the video,\n",
        "    # then we break the loop\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Getting spatial dimensions of the frame\n",
        "    # we do it only once from the very beginning\n",
        "    # all other frames have the same dimension\n",
        "    if w is None or h is None:\n",
        "        # Slicing from tuple only first two elements\n",
        "        h, w = frame.shape[:2]\n",
        "\n",
        "    \"\"\"\n",
        "    Start of:\n",
        "    Getting blob from current frame\n",
        "    \"\"\"\n",
        "\n",
        "    # Getting blob from current frame\n",
        "    # The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob from current\n",
        "    # frame after mean subtraction, normalizing, and RB channels swapping\n",
        "    # Resulted shape has number of frames, number of channels, width and height\n",
        "    # E.G.:\n",
        "    # blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
        "                                 swapRB=True, crop=False)\n",
        "\n",
        "    \"\"\"\n",
        "    End of:\n",
        "    Getting blob from current frame\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Start of:\n",
        "    Implementing Forward pass\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementing forward pass with our blob and only through output layers\n",
        "    # Calculating at the same time, needed time for forward pass\n",
        "    network.setInput(blob)  # setting blob as input to the network\n",
        "    start = time.time()\n",
        "    output_from_network = network.forward(layers_names_output)\n",
        "    end = time.time()\n",
        "\n",
        "    # Increasing counters for frames and total time\n",
        "    f += 1\n",
        "    t += end - start\n",
        "\n",
        "    # Showing spent time for single current frame\n",
        "    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n",
        "\n",
        "    \"\"\"\n",
        "    End of:\n",
        "    Implementing Forward pass\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Start of:\n",
        "    Getting bounding boxes\n",
        "    \"\"\"\n",
        "\n",
        "    # Preparing lists for detected bounding boxes,\n",
        "    # obtained confidences and class's number\n",
        "    bounding_boxes = []\n",
        "    confidences = []\n",
        "    class_numbers = []\n",
        "\n",
        "    # Going through all output layers after feed forward pass\n",
        "    for result in output_from_network:\n",
        "        # Going through all detections from current output layer\n",
        "        for detected_objects in result:\n",
        "            # Getting 80 classes' probabilities for current detected object\n",
        "            scores = detected_objects[5:]\n",
        "            # Getting index of the class with the maximum value of probability\n",
        "            class_current = np.argmax(scores)\n",
        "            # Getting value of probability for defined class\n",
        "            confidence_current = scores[class_current]\n",
        "\n",
        "            # # Check point\n",
        "            # # Every 'detected_objects' numpy array has first 4 numbers with\n",
        "            # # bounding box coordinates and rest 80 with probabilities\n",
        "            #  # for every class\n",
        "            # print(detected_objects.shape)  # (85,)\n",
        "\n",
        "            # Eliminating weak predictions with minimum probability\n",
        "            if confidence_current > probability_minimum:\n",
        "                # Scaling bounding box coordinates to the initial frame size\n",
        "                # YOLO data format keeps coordinates for center of bounding box\n",
        "                # and its current width and height\n",
        "                # That is why we can just multiply them elementwise\n",
        "                # to the width and height\n",
        "                # of the original frame and in this way get coordinates for center\n",
        "                # of bounding box, its width and height for original frame\n",
        "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
        "\n",
        "                # Now, from YOLO data format, we can get top left corner coordinates\n",
        "                # that are x_min and y_min\n",
        "                x_center, y_center, box_width, box_height = box_current\n",
        "                x_min = int(x_center - (box_width / 2))\n",
        "                y_min = int(y_center - (box_height / 2))\n",
        "\n",
        "                # Adding results into prepared lists\n",
        "                bounding_boxes.append([x_min, y_min,\n",
        "                                       int(box_width), int(box_height)])\n",
        "                confidences.append(float(confidence_current))\n",
        "                class_numbers.append(class_current)\n",
        "\n",
        "    \"\"\"\n",
        "    End of:\n",
        "    Getting bounding boxes\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Start of:\n",
        "    Non-maximum suppression\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementing non-maximum suppression of given bounding boxes\n",
        "    # With this technique we exclude some of bounding boxes if their\n",
        "    # corresponding confidences are low or there is another\n",
        "    # bounding box for this region with higher confidence\n",
        "\n",
        "    # It is needed to make sure that data type of the boxes is 'int'\n",
        "    # and data type of the confidences is 'float'\n",
        "    # https://github.com/opencv/opencv/issues/12789\n",
        "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,\n",
        "                               probability_minimum, threshold)\n",
        "\n",
        "    \"\"\"\n",
        "    End of:\n",
        "    Non-maximum suppression\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Start of:\n",
        "    Drawing bounding boxes and labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Checking if there is at least one detected object\n",
        "    # after non-maximum suppression\n",
        "    if len(results) > 0:\n",
        "        # Going through indexes of results\n",
        "        for i in results.flatten():\n",
        "            # Getting current bounding box coordinates,\n",
        "            # its width and height\n",
        "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
        "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
        "\n",
        "            # Preparing colour for current bounding box\n",
        "            # and converting from numpy array to list\n",
        "            colour_box_current = colours[class_numbers[i]].tolist()\n",
        "\n",
        "            # # # Check point\n",
        "            # print(type(colour_box_current))  # <class 'list'>\n",
        "            # print(colour_box_current)  # [172 , 10, 127]\n",
        "\n",
        "            # Drawing bounding box on the original current frame\n",
        "            cv2.rectangle(frame, (x_min, y_min),\n",
        "                          (x_min + box_width, y_min + box_height),\n",
        "                          colour_box_current, 2)\n",
        "\n",
        "            # Preparing text with label and confidence for current bounding box\n",
        "            text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n",
        "                                                   confidences[i])\n",
        "\n",
        "            # Putting text with label and confidence on the original image\n",
        "            cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n",
        "\n",
        "    \"\"\"\n",
        "    End of:\n",
        "    Drawing bounding boxes and labels\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Start of:\n",
        "    Writing processed frame into the file\n",
        "    \"\"\"\n",
        "\n",
        "    # Initializing writer\n",
        "    # we do it only once from the very beginning\n",
        "    # when we get spatial dimensions of the frames\n",
        "    if writer is None:\n",
        "        # Constructing code of the codec\n",
        "        # to be used in the function VideoWriter\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "        # Writing current processed frame into the video file\n",
        "        # Pay attention! If you're using Windows, yours path might looks like:\n",
        "        # r'videos\\result-traffic-cars.mp4'\n",
        "        # or:\n",
        "        # 'videos\\\\result-traffic-cars.mp4'\n",
        "        writer = cv2.VideoWriter('videos/result-traffic-cars.mp4', fourcc, 30,\n",
        "                                 (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "    # Write processed current frame to the file\n",
        "    writer.write(frame)\n",
        "\n",
        "    \"\"\"\n",
        "    End of:\n",
        "    Writing processed frame into the file\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48yjDKHve8io"
      },
      "source": [
        "# Printing final results\n",
        "print()\n",
        "print('Total number of frames', f)\n",
        "print('Total amount of time {:.5f} seconds'.format(t))\n",
        "print('FPS:', round((f / t), 1))\n",
        "\n",
        "\n",
        "# Releasing video reader and writer\n",
        "video.release()\n",
        "writer.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}